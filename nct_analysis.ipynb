{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cee1660",
   "metadata": {},
   "source": [
    "Network Control Theory _ Alignment & Control Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a152959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sEEG ↔ Centroidi (B0/EPI/MNI) — Verifica, Allineamento, Control Set, Control Energy\n",
    "# Pipeline completa:\n",
    "# 1) Caricamento e QC (bounding box, distanze)\n",
    "# 2) Verifica midpoint bipolari\n",
    "# 3) Allineamento (min–max affine + ICP rigido) se necessario\n",
    "# 4) Costruzione control set (k-NN su B0)\n",
    "# 5) QC distanze + bundle Excel\n",
    "# 6) Control Energy (Procedure 1) per banda + salvataggi \n",
    "# 7) Post-analisi ed estrazione principali controller per banda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3c0d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutti i file trovati ✅\n"
     ]
    }
   ],
   "source": [
    "# ## IMPORT LIBRERIE (paper) & PATH BASE\n",
    "# --- standard/core ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.io import loadmat\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "# opzionali / velocità / grafica: scikit-learn per KDTree \n",
    "# (altrimenti fallback NumPy)\n",
    "try:\n",
    "    from sklearn.neighbors import KDTree\n",
    "    HAVE_SKLEARN = True\n",
    "except Exception:\n",
    "    HAVE_SKLEARN = False\n",
    "\n",
    "# --- plotting ---\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\"font.size\": 10})\n",
    "plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "\n",
    "# --- neuroimaging ---\n",
    "try:\n",
    "    from nilearn import datasets, plotting  # richiede nibabel\n",
    "except Exception:\n",
    "    datasets = plotting = None\n",
    "# carica atlanti/surface e plot cerebrali.\n",
    "\n",
    "# --- nctpy (dal toolkit del paper, Procedure 1) ---\n",
    "from nctpy.energies import integrate_u, get_control_inputs\n",
    "from nctpy.pipelines import ComputeControlEnergy, ComputeOptimizedControlEnergy\n",
    "from nctpy.metrics import ave_control\n",
    "from nctpy.utils import (\n",
    "    matrix_normalization,\n",
    "    convert_states_str2int,\n",
    "    normalize_state,\n",
    "    normalize_weights,\n",
    "    get_null_p,\n",
    "    get_fdr_p,\n",
    ")\n",
    "\n",
    "from nctpy.plotting import (\n",
    "    roi_to_vtx,\n",
    "    null_plot,\n",
    "    surface_plot,\n",
    "    add_module_lines,  \n",
    ")\n",
    "# ATTENZIONE: null models è un modulo interno al repository degli autori, non un pacchetto PyPI “standard”. \n",
    "# Posso scegliere un strada alternativa richiamando surrogati equivalenti da librerie note.\n",
    "\n",
    "# Path base\n",
    "try:\n",
    "    ROOT = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Notebook / VS Code Interactive\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        ip = get_ipython()\n",
    "        # current working dir of the kernel/session\n",
    "        ROOT = Path(ip.run_line_magic('pwd', ''))\n",
    "    except Exception:\n",
    "        ROOT = Path.cwd()\n",
    "\n",
    "BASE = ROOT / \"NCT_analysis\\data\"\n",
    "\n",
    "B0_TXT    = BASE / \"centroidsB0_850ROIs.txt\"\n",
    "EPI_TXT   = BASE / \"centroidsEPI_850ROIs.txt\"\n",
    "MNI_XLSX  = BASE / \"centroidsMNI2mm_850ROIs.xlsx\"\n",
    "SEEG_MONO = BASE / \"sEEG_3D_contacts_locs.mat\"\n",
    "SEEG_BI   = BASE / \"sEEG_3D_BIPOLARcontacts_locs.mat\"\n",
    "\n",
    "# Cartella per gli output\n",
    "OUT = ROOT / \"output\"\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "# Controllo dei file:\n",
    "for p in [B0_TXT, EPI_TXT, MNI_XLSX, SEEG_MONO, SEEG_BI]:\n",
    "    assert p.exists(), f\"File mancante: {p}\"\n",
    "print(\"Tutti i file trovati ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df1074f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## a) Loader & Utilità geometriche\n",
    "def _check_xyz_shape(xyz: np.ndarray, name: str):\n",
    "    if not (isinstance(xyz, np.ndarray) and xyz.ndim == 2 and xyz.shape[1] == 3 and xyz.shape[0] > 0):\n",
    "        raise ValueError(f\"{name}: atteso (N,3) con N>0, trovato {getattr(xyz, 'shape', None)}\")\n",
    "\n",
    "def _to_float_series(s: pd.Series):\n",
    "    # Supporta e prova a convertire numeri come stringhe con possibile virgola decimale\n",
    "    return pd.to_numeric(s.astype(str).str.replace(\",\", \".\", regex=False), errors=\"coerce\")\n",
    "\n",
    "def load_centroids_txt(path: Path):\n",
    "    \"\"\"\n",
    "    Legge i centroidi da TXT con righe tipo: id x y z \n",
    "    Accetta spazi/tab/virgole, salta righe vuote o commenti (#).\n",
    "    Ritorna: ids (N,), xyz (N,3)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=r\"[\\s,;]+\",\n",
    "        engine=\"python\",\n",
    "        header=None,\n",
    "        comment=\"#\",\n",
    "        skip_blank_lines=True\n",
    "    )\n",
    "    # Converto tutto in numerico e rimuovo colonne/righe interamente vuote (pulizia)\n",
    "    for c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(axis=1, how=\"all\").dropna(how=\"any\")\n",
    "    if df.shape[1] < 4:\n",
    "        raise ValueError(f\"{path} non ha 4 colonne numeriche (id x y z). Letto shape={df.shape}\")\n",
    "    df = df.iloc[:, :4]\n",
    "    ids = df.iloc[:, 0].astype(int).to_numpy()\n",
    "    xyz = df.iloc[:, 1:4].astype(float).to_numpy()\n",
    "    _check_xyz_shape(xyz, f\"Centroidi TXT: {path.name}\")\n",
    "    return ids, xyz\n",
    "\n",
    "def load_mni_centroids_excel(path: Path):\n",
    "    \"\"\"\n",
    "    Legge centroidi MNI da Excel; cerca colonne 'R/L', 'A/P', 'S/I' o \n",
    "    fallback su ultime 3 colonne numeriche.\n",
    "    Ritorna xyz (N,3)\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    xcol = cols.get(\"r/l\") or cols.get(\"x\")\n",
    "    ycol = cols.get(\"a/p\") or cols.get(\"y\")\n",
    "    zcol = cols.get(\"s/i\") or cols.get(\"z\")\n",
    "    if not (xcol and ycol and zcol):\n",
    "        # fallback: prendi le ultime 3 colonne numeriche\n",
    "        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "        if len(num_cols) < 3:\n",
    "            raise ValueError(f\"Impossibile trovare 3 colonne numeriche in {path}\")\n",
    "        xcol, ycol, zcol = num_cols[-3:]\n",
    "    xyz = df[[xcol, ycol, zcol]].astype(float).to_numpy()\n",
    "    _check_xyz_shape(xyz, f\"Centroidi MNI: {path.name}\")\n",
    "    return xyz\n",
    "\n",
    "def parse_contacts_mat(path: Path, preferred_key: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carica contatti da .mat in modo robusto.\n",
    "    - Se preferred_key è fornita, la usa (es. 'sEEG_3D_contacts_locs').\n",
    "    - Altrimenti cerca la prima chiave non 'dunder' con tabella NxK.\n",
    "    Seleziona le 3 colonne più 'numericabili' come x,y,z e usa la prima non-numerica \n",
    "    come label (se esiste).\n",
    "    Ritorna DataFrame con colonne: label, x, y, z\n",
    "    \"\"\"\n",
    "    data = loadmat(path, squeeze_me=True, struct_as_record=False)\n",
    "    keys = [k for k in data.keys() if not k.startswith(\"__\")]\n",
    "    key = preferred_key if (preferred_key in keys) else (keys[0] if keys else None)\n",
    "    if key is None:\n",
    "        raise ValueError(f\"Nessuna variabile utile trovata in {path}\")\n",
    "    arr = data[key]\n",
    "    df = pd.DataFrame(arr)\n",
    "\n",
    "    # valuto quali colonne possono diventare numeriche\n",
    "    numeric_score = []\n",
    "    for c in df.columns:\n",
    "        conv = _to_float_series(df[c])\n",
    "        numeric_score.append((conv.notna().sum(), c))\n",
    "    numeric_score.sort(reverse=True)\n",
    "    num_cols = [c for _, c in numeric_score[:3]]\n",
    "    xyz = pd.concat([_to_float_series(df[c]) for c in num_cols], axis=1).to_numpy(dtype=float)\n",
    "\n",
    "    # etichetta = prima colonna non numerica rimanente (se esiste)\n",
    "    label_cols = [c for c in df.columns if c not in num_cols]\n",
    "    if label_cols:\n",
    "        labels = df[label_cols[0]].astype(str).to_numpy()\n",
    "    else:\n",
    "        labels = np.array([f\"C{i+1}\" for i in range(len(df))], dtype=str)\n",
    "\n",
    "    # rimuovo righe con NaN\n",
    "    keep = ~np.isnan(xyz).any(axis=1)\n",
    "    xyz, labels = xyz[keep], labels[keep]\n",
    "    _check_xyz_shape(xyz, f\"Contatti: {path.name}\")\n",
    "    return pd.DataFrame({\"label\": labels, \"x\": xyz[:,0], \"y\": xyz[:,1], \"z\": xyz[:,2]})\n",
    "\n",
    " # ------------------ Utilità geometriche ----------------- \n",
    "\n",
    "def bbox(arr: np.ndarray):\n",
    "    \"\"\"Restituisce bounding box e conta punti.\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    mins, maxs = arr.min(axis=0), arr.max(axis=0)\n",
    "    return dict(\n",
    "        min_x=float(mins[0]), max_x=float(maxs[0]),\n",
    "        min_y=float(mins[1]), max_y=float(maxs[1]),\n",
    "        min_z=float(mins[2]), max_z=float(maxs[2]),\n",
    "        n=int(arr.shape[0])\n",
    "    )\n",
    "\n",
    "def nearest_dists(points: np.ndarray, centroids: np.ndarray):\n",
    "    \"\"\"Distanza e indice del nearest neighbor di ogni point rispetto a centroids.\"\"\"\n",
    "    if HAVE_SKLEARN:\n",
    "        tree = KDTree(centroids)\n",
    "        d, idx = tree.query(points, k=1)\n",
    "        return d.ravel(), idx.ravel()\n",
    "    # Fallback NumPy\n",
    "    P = points[:, None, :]           # (N,1,3)\n",
    "    C = centroids[None, :, :]        # (1,M,3)\n",
    "    d2 = np.sum((P - C)**2, axis=2)  # (N,M)\n",
    "    idx = np.argmin(d2, axis=1)\n",
    "    d = np.sqrt(d2[np.arange(points.shape[0]), idx])\n",
    "    return d, idx\n",
    "\n",
    "\n",
    "def two_nearest(points: np.ndarray, cloud: np.ndarray):\n",
    "    \"\"\"Per ogni point, trova i 2 vicini più prossimi in cloud. \n",
    "    Ritorna (dists (N,2), idx (N,2)).\"\"\"\n",
    "    if HAVE_SKLEARN:\n",
    "        tree = KDTree(cloud)\n",
    "        d, idx = tree.query(points, k=2)\n",
    "        return d, idx\n",
    "    # fallback NumPy\n",
    "    P = points[:, None, :]\n",
    "    C = cloud[None, :, :]\n",
    "    d2 = np.sum((P - C) ** 2, axis=2)      # (N, M)\n",
    "    idx1 = np.argmin(d2, axis=1)           # (N,)\n",
    "    # maschera per trovare il 2° vicino\n",
    "    mask = np.ones_like(d2, dtype=bool)\n",
    "    mask[np.arange(points.shape[0]), idx1] = False\n",
    "    d2_masked = np.where(mask, d2, np.inf)\n",
    "    idx2 = np.argmin(d2_masked, axis=1)    # (N,)\n",
    "    d1 = np.sqrt(d2[np.arange(points.shape[0]), idx1])\n",
    "    d2v = np.sqrt(d2[np.arange(points.shape[0]), idx2])\n",
    "    d = np.stack([d1, d2v], axis=1)\n",
    "    idx = np.stack([idx1, idx2], axis=1)\n",
    "    return d, idx\n",
    "\n",
    "\n",
    "def summary_line(name, d):\n",
    "    return f\"{name}: N={d.size}, median={np.median(d):.3f}, mean={np.mean(d):.3f}, min={np.min(d):.3f}, max={np.max(d):.3f}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92852bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## b) Allineamento: min-max affine (pre-asse) + ICP rigido\n",
    "def minmax_affine(X: np.ndarray, T: np.ndarray):\n",
    "    \"\"\"\n",
    "    Allinea grossolanamente X a T mappando il bounding box per asse:\n",
    "    Y = X * scale + offset, dove scale/offset sono vettori (3,)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    T = np.asarray(T, float)\n",
    "    minX, maxX = X.min(axis=0), X.max(axis=0)\n",
    "    minT, maxT = T.min(axis=0), T.max(axis=0)\n",
    "    scale = (maxT - minT) / (maxX - minX + 1e-12)\n",
    "    offset = minT - minX * scale\n",
    "    Y = X * scale + offset\n",
    "    return Y, scale, offset\n",
    "\n",
    "\n",
    "def kabsch_rigid(A: np.ndarray, B: np.ndarray):\n",
    "    \"\"\"\n",
    "    Stima trasformazione rigida (R,t) che porta A → B dato un matching punto-a-punto.\n",
    "    R è 3x3 ortonormale; t è (3,)\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, float); B = np.asarray(B, float)\n",
    "    cA = A.mean(axis=0); cB = B.mean(axis=0)\n",
    "    AA = A - cA; BB = B - cB\n",
    "    H = AA.T @ BB\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = Vt.T @ U.T\n",
    "    # gestisci riflessione\n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[-1, :] *= -1\n",
    "        R = Vt.T @ U.T\n",
    "    t = cB - R @ cA\n",
    "    return R, t\n",
    "\n",
    "\n",
    "def icp_rigid(X: np.ndarray, T: np.ndarray, max_iter=50, tol=1e-5):\n",
    "    \"\"\"\n",
    "    ICP semplice: stima R,t tali che R*X + t ≈ T.\n",
    "    Inizializza con identità (si consiglia di passare X già grossolanamente allineato con minmax_affine).\n",
    "    \"\"\"\n",
    "    Xc = X.copy()\n",
    "    prev_err = None\n",
    "    for it in range(max_iter):\n",
    "        # 1) nearest nei target\n",
    "        d, idx = nearest_dists(Xc, T)\n",
    "        matched_T = T[idx]\n",
    "        # 2) kabsch su corrispondenze\n",
    "        R, t = kabsch_rigid(Xc, matched_T)\n",
    "        Xc_new = (R @ Xc.T).T + t\n",
    "        # 3) errore e stop\n",
    "        err = np.median(np.linalg.norm(Xc_new - matched_T, axis=1))\n",
    "        Xc = Xc_new\n",
    "        if prev_err is not None and abs(prev_err - err) < tol:\n",
    "            break\n",
    "        prev_err = err\n",
    "    return Xc, dict(median_err=float(prev_err or err), it=it+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07b3fc",
   "metadata": {},
   "source": [
    "Caricamento dati, Quick Check, Verifica spazi, Midpoint bipolari, Allineamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "681fcc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Bounding boxes (unità dei file):\n",
      "           set      min_x     max_x      min_y     max_y     min_z     max_z   n\n",
      "  B0 centroids  23.039216 87.866667   8.184080 85.152672 20.052356 70.195122 850\n",
      " EPI centroids  21.476190 73.230769   7.707317 71.888889  8.741071 51.130435 850\n",
      " MNI centroids  12.014925 77.220000  11.221311 96.903846 13.945545 75.250000 850\n",
      "sEEG monopolar -33.867000  1.591000 -33.723000 13.342000 13.132000 71.684000  86\n",
      "  sEEG bipolar -33.478000  1.528500 -33.182500 13.298500 15.075500 69.966000  77\n",
      "\n",
      "# Distanze nearest-centroid (pre-allineamento):\n",
      "Monopolar vs B0: N=86, median=61.512, mean=60.995, min=48.321, max=73.800\n",
      "Bipolar   vs B0: N=77, median=61.617, mean=60.975, min=48.648, max=73.229\n",
      "Monopolar vs MNI: N=86, median=56.967, mean=55.654, min=40.674, max=68.487\n",
      "Bipolar   vs MNI: N=77, median=57.054, mean=55.643, min=41.324, max=67.601\n",
      "Monopolar vs EPI: N=86, median=60.419, mean=59.719, min=48.557, max=73.131\n",
      "Bipolar   vs EPI: N=77, median=60.708, mean=59.693, min=48.844, max=72.127\n",
      "\n",
      "# Giudizio euristico di spazio:\n",
      "Stesso spazio dei centroidi B0?  NO\n",
      "Stesso spazio dei centroidi MNI? NO\n",
      "\n",
      "# Bipolari ~ midpoint di 2 monopolar più vicini:\n",
      "median dist to midpoint = 0.000 ; median ratio = 0.000 (≈0 se esatto midpoint; ~0.5 se sul contatto)\n",
      "\n",
      "# Allineamento verso B0\n",
      "Monopolar pre  → median nn-dist = 61.512\n",
      "Monopolar post → median nn-dist = 3.005 (ICP iters=33, med.err=3.005)\n",
      "Bipolar   pre  → median nn-dist = 61.617\n",
      "Bipolar   post → median nn-dist = 3.189 (ICP iters=23, med.err=3.189)\n",
      "\n",
      "# Allineamento verso MNI\n",
      "Monopolar pre  → median nn-dist = 56.967\n",
      "Monopolar post → median nn-dist = 3.421 (ICP iters=42, med.err=3.421)\n",
      "Bipolar   pre  → median nn-dist = 57.054\n",
      "Bipolar   post → median nn-dist = 3.419 (ICP iters=37, med.err=3.419)\n",
      "\n",
      "== FINE VERIFICA/ALLINEAMENTO ==\n",
      "Output salvati in: C:\\Users\\barba\\Visual Studio - Github\\NCT\\output\n"
     ]
    }
   ],
   "source": [
    "THR_ALIGN = 10.0  # soglia euristica (mm) per dire \"stesso spazio\": \n",
    "                  # se mediana distanza <= 10 → \"ok stesso spazio\"\n",
    "\n",
    "# 1) Caricamento dati\n",
    "b0_ids, b0_xyz  = load_centroids_txt(B0_TXT)    # recupero anche gli ID (coerenti con l'ordine del connettoma strutturale)\n",
    "epi_ids, epi_xyz = load_centroids_txt(EPI_TXT)\n",
    "mni_xyz   = load_mni_centroids_excel(MNI_XLSX)\n",
    "\n",
    "mono_df = parse_contacts_mat(SEEG_MONO, preferred_key=\"sEEG_3D_contacts_locs\")\n",
    "bi_df   = parse_contacts_mat(SEEG_BI,   preferred_key=\"sEEG_3D_BIPOLARcontacts_locs\")\n",
    "mono_pts = mono_df[[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "bi_pts   = bi_df[[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "\n",
    "# 2) Bounding boxes (QC rapido)\n",
    "bb_tbl = pd.DataFrame([\n",
    "    {\"set\": \"B0 centroids\",   **bbox(b0_xyz)},\n",
    "    {\"set\": \"EPI centroids\",  **bbox(epi_xyz)},\n",
    "    {\"set\": \"MNI centroids\",  **bbox(mni_xyz)},\n",
    "    {\"set\": \"sEEG monopolar\", **bbox(mono_pts)},\n",
    "    {\"set\": \"sEEG bipolar\",   **bbox(bi_pts)},\n",
    "])\n",
    "print(\"\\n# Bounding boxes (unità dei file):\")\n",
    "print(bb_tbl.to_string(index=False))\n",
    "bb_tbl.to_csv(OUT / \"bbox_summary.csv\", index=False)\n",
    "\n",
    "# 3) Verifica spazio coordinate: distanze ai centroidi (pre-allineamento)\n",
    "d_mono_b0, _ = nearest_dists(mono_pts, b0_xyz)\n",
    "d_bi_b0,   _ = nearest_dists(bi_pts,   b0_xyz)\n",
    "d_mono_mni, _ = nearest_dists(mono_pts, mni_xyz)\n",
    "d_bi_mni,   _ = nearest_dists(bi_pts,   mni_xyz)\n",
    "d_mono_epi, _ = nearest_dists(mono_pts, epi_xyz)  # facoltativo/diagnostico\n",
    "d_bi_epi,   _ = nearest_dists(bi_pts,   epi_xyz)\n",
    "\n",
    "print(\"\\n# Distanze nearest-centroid (pre-allineamento):\")\n",
    "print(summary_line(\"Monopolar vs B0\",  d_mono_b0))\n",
    "print(summary_line(\"Bipolar   vs B0\",  d_bi_b0))\n",
    "print(summary_line(\"Monopolar vs MNI\", d_mono_mni))\n",
    "print(summary_line(\"Bipolar   vs MNI\", d_bi_mni))\n",
    "print(summary_line(\"Monopolar vs EPI\", d_mono_epi))\n",
    "print(summary_line(\"Bipolar   vs EPI\", d_bi_epi))\n",
    "\n",
    "aligned_b0  = (np.median(d_mono_b0) <= THR_ALIGN) and (np.median(d_bi_b0) <= THR_ALIGN)\n",
    "aligned_mni = (np.median(d_mono_mni) <= THR_ALIGN) and (np.median(d_bi_mni) <= THR_ALIGN)\n",
    "print(\"\\n# Giudizio euristico di spazio:\")\n",
    "print(f\"Stesso spazio dei centroidi B0?  {'SI' if aligned_b0  else 'NO'}\")\n",
    "print(f\"Stesso spazio dei centroidi MNI? {'SI' if aligned_mni else 'NO'}\")\n",
    "\n",
    "# 4) Verifica: i bipolari sono midpoint dei 2 monopolari più vicini?\n",
    "d2, idx2 = two_nearest(bi_pts, mono_pts)  # per ogni bi → due mono più vicini\n",
    "midpoints = (mono_pts[idx2[:, 0]] + mono_pts[idx2[:, 1]]) / 2.0\n",
    "dist_mid = np.linalg.norm(bi_pts - midpoints, axis=1)     # quanto il bipolare sta vicino al midpoint?\n",
    "edge_len = np.linalg.norm(mono_pts[idx2[:, 0]] - mono_pts[idx2[:, 1]], axis=1)  # distanza tra i due mono\n",
    "ratio = dist_mid / (edge_len + 1e-12)                     # normalizzato alla spaziatura\n",
    "mid_df = pd.DataFrame({\n",
    "    \"bipolar_label\": bi_df[\"label\"],\n",
    "    \"mono1_idx\": idx2[:, 0] + 1,  # +1 se vuoi numerazione 1-based\n",
    "    \"mono2_idx\": idx2[:, 1] + 1,\n",
    "    \"dist_to_midpoint\": dist_mid,\n",
    "    \"mono_pair_distance\": edge_len,\n",
    "    \"midpoint_ratio\": ratio\n",
    "})\n",
    "mid_df.to_csv(OUT / \"check_bipolar_midpoint.csv\", index=False)\n",
    "print(\"\\n# Bipolari ~ midpoint di 2 monopolar più vicini:\")\n",
    "print(f\"median dist to midpoint = {np.median(dist_mid):.3f} ; \"\n",
    "      f\"median ratio = {np.median(ratio):.3f} (≈0 se esatto midpoint; ~0.5 se sul contatto)\")\n",
    "\n",
    "# 5) Se NON allineati, eseguo allineamento verso B0 e/o MNI\n",
    "def align_to_target(name: str, target_xyz: np.ndarray):\n",
    "    \"\"\"\n",
    "    Allinea mono/bi ai centroidi 'target_xyz' in due step:\n",
    "      (A) affine min–max per asse\n",
    "      (B) ICP rigido di rifinitura\n",
    "    Salva CSV e ritorna mediana distanza prima/dopo.\n",
    "    \"\"\"\n",
    "    # prima delle trasformazioni\n",
    "    d_mono_pre, _ = nearest_dists(mono_pts, target_xyz)\n",
    "    d_bi_pre,   _ = nearest_dists(bi_pts,   target_xyz)\n",
    "\n",
    "    # (A) min–max affine\n",
    "    mono_mm, s_mono, o_mono = minmax_affine(mono_pts, target_xyz)\n",
    "    bi_mm,   s_bi,   o_bi   = minmax_affine(bi_pts,   target_xyz)\n",
    "\n",
    "    # (B) ICP rigido (su output affine)\n",
    "    mono_icp, info_mono = icp_rigid(mono_mm, target_xyz, max_iter=50, tol=1e-5)\n",
    "    bi_icp,   info_bi   = icp_rigid(bi_mm,   target_xyz, max_iter=50, tol=1e-5)\n",
    "\n",
    "    # distanze finali\n",
    "    d_mono_post, _ = nearest_dists(mono_icp, target_xyz)\n",
    "    d_bi_post,   _ = nearest_dists(bi_icp,   target_xyz)\n",
    "\n",
    "    # salvataggi\n",
    "    pd.DataFrame(mono_icp, columns=[\"x\",\"y\",\"z\"]).to_csv(OUT / f\"sEEG_monopolar_aligned_to_{name}.csv\", index=False)\n",
    "    pd.DataFrame(bi_icp,   columns=[\"x\",\"y\",\"z\"]).to_csv(OUT / f\"sEEG_bipolar_aligned_to_{name}.csv\",   index=False)\n",
    "\n",
    "    # breve log\n",
    "    print(f\"\\n# Allineamento verso {name}\")\n",
    "    print(f\"Monopolar pre  → median nn-dist = {np.median(d_mono_pre):.3f}\")\n",
    "    print(f\"Monopolar post → median nn-dist = {np.median(d_mono_post):.3f} (ICP iters={info_mono['it']}, med.err={info_mono['median_err']:.3f})\")\n",
    "    print(f\"Bipolar   pre  → median nn-dist = {np.median(d_bi_pre):.3f}\")\n",
    "    print(f\"Bipolar   post → median nn-dist = {np.median(d_bi_post):.3f} (ICP iters={info_bi['it']}, med.err={info_bi['median_err']:.3f})\")\n",
    "\n",
    "    return dict(\n",
    "        mono_pre=np.median(d_mono_pre), mono_post=np.median(d_mono_post),\n",
    "        bi_pre=np.median(d_bi_pre),     bi_post=np.median(d_bi_post)\n",
    "    )\n",
    "\n",
    "# Esegui allineamenti solo se necessario\n",
    "if not aligned_b0:\n",
    "    res_b0 = align_to_target(\"B0\", b0_xyz)\n",
    "else:\n",
    "    print(\"\\nContatti già nel sistema B0 (entro soglia). Non eseguo allineamento verso B0.\")\n",
    "\n",
    "if not aligned_mni:\n",
    "    res_mni = align_to_target(\"MNI\", mni_xyz)\n",
    "else:\n",
    "    print(\"Contatti già nel sistema MNI (entro soglia). Non eseguo allineamento verso MNI.\")\n",
    "\n",
    "print(\"\\n== FINE VERIFICA/ALLINEAMENTO ==\")\n",
    "print(f\"Output salvati in: {OUT.resolve()}\")\n",
    "\n",
    "\n",
    "# Come leggere l'output:\n",
    "# output/bbox_summary.csv → range degli assi per ogni set \n",
    "#                           (fa vedere subito se le scale/origini sono comparabili).\n",
    "\n",
    "# stampa a console delle distanze: se la mediana sEEG→B0 (o →MNI) è ≤ ~10 \n",
    "#                                   → di solito stesso spazio.\n",
    "\n",
    "# output/check_bipolar_midpoint.csv → per ogni bipolare:\n",
    "#   dist_to_midpoint: distanza dal punto medio dei 2 monopolari più vicini; ideale ≈ 0.\n",
    "#   midpoint_ratio = dist_to_midpoint / distanza_mono_pair: normalizzato; ideale ≈ 0.\n",
    "\n",
    "# Se serve l’allineamento: \n",
    "#   →  sEEG_monopolar_aligned_to_B0.csv / sEEG_bipolar_aligned_to_B0.csv (e le versioni _to_MNI)\n",
    "#       con le coordinate allineate. Le mediane delle distanze “post” dovrebbero scendere marcatamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1850ca78",
   "metadata": {},
   "source": [
    "COSTRUZIONE CONTROL SET (ROI più vicine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "950ffbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[control set] Uso MONO allineati: sEEG_monopolar_aligned_to_B0.csv\n",
      "[control set] Uso BI allineati: sEEG_bipolar_aligned_to_B0.csv\n",
      "[CHECK] MONO per-contatto: 86 righe (contatti unici=86)\n",
      "[CHECK] BI per-contatto: 77 righe (contatti unici=77)\n",
      "\n",
      "== MAPPE & CONTROL SET (PER-CONTATTO, CON DUPLICATI) COSTRUITI ==\n",
      "- Mappe complete:      controlset_map_monopolar_k1.csv | controlset_map_bipolar_k1.csv\n",
      "- Mappe top-1:         contact2roi_MONO_top1.csv | contact2roi_BI_top1.csv\n",
      "- Control set MONO PC: control_set_ROIs_B0_MONO_top1_perContact.txt (#righe=86, contatti unici=86)\n",
      "- Control set BI   PC: control_set_ROIs_B0_BI_top1_perContact.txt   (#righe=77, contatti unici=77)\n",
      ">> Per la control energy usa: control_set_ROIs_B0_MONO_top1_perContact.txt\n"
     ]
    }
   ],
   "source": [
    "# Obiettivo: associare a ciascun contatto sEEG la ROI più vicina (centroide B0) \n",
    "# (K=1, distanza euclidea minima) e costruire il control set mantenendo i duplicati\n",
    "# (uno-per-contatto). Userò il control set MONO nella control energy.\n",
    "\n",
    "# --- Parametri principali  ---\n",
    "K_NEAREST = 1            # quante ROI vicine devono essere prese per ogni contatto (k-NN)\n",
    "MAX_DIST = None          # soglia massima di distanza (stesse unità dei centroidi). Se None, nessun filtro.\n",
    "\n",
    "# --- Si preferiscono i contatti allineati in B0 salvati ---\n",
    "MONO_ALIGNED_CSV = OUT / \"sEEG_monopolar_aligned_to_B0.csv\"\n",
    "BI_ALIGNED_CSV   = OUT / \"sEEG_bipolar_aligned_to_B0.csv\"\n",
    "\n",
    "# --- Helper: k-NN (usa KDTree caricato sopra; altrimenti fallback NumPy) ---\n",
    "def knn_indices_dists(points: np.ndarray, refs: np.ndarray, k: int):\n",
    "    \"\"\"\n",
    "    Dato un insieme di punti (points) e i centroidi (refs), \n",
    "    restituisce (d, idx) con shape (N, k): per ogni point[i], distanze e indici dei k\n",
    "    centroidi più vicini in 'refs'.\n",
    "    d = distanze ai k più vicini\n",
    "    idx = indici di quei centroidi\n",
    "    \"\"\"\n",
    "    if 'HAVE_SKLEARN' in globals() and HAVE_SKLEARN:\n",
    "        tree = KDTree(refs)\n",
    "        d, idx = tree.query(points, k=k)\n",
    "        return d, idx\n",
    "    # Fallback O(NM): va benissimo per N e M ~ 10^3\n",
    "    P = points[:, None, :]             # (N,1,3)\n",
    "    R = refs[None, :, :]               # (1,M,3)\n",
    "    d2 = np.sum((P - R)**2, axis=2)    # (N,M)\n",
    "    idx_sorted = np.argsort(d2, axis=1)[:, :k]\n",
    "    rows = np.arange(points.shape[0])[:, None]\n",
    "    d = np.sqrt(d2[rows, idx_sorted])\n",
    "    return d, idx_sorted\n",
    "\n",
    "######################################################################################################\n",
    "##################################### COSTRUZIONE CONTROL SET ########################################\n",
    "######################################################################################################\n",
    "# --- 1) Recupero ID+centroidi B0 (gli ID servono per allinearsi al connettoma) ---\n",
    "#    Nel blocco precedente ho fatto: \"b0_ids, b0_xyz = load_centroids_txt(B0_TXT)\",\n",
    "#    dove recupero anche gli ID (coerenti con l'ordine del connettoma strutturale).\n",
    "\n",
    "# --- 2) Considero i contatti sEEG (monopolar & bipolar) memorizzati nel blocco precedente\n",
    "# mono_df = parse_contacts_mat(SEEG_MONO, preferred_key=\"sEEG_3D_contacts_locs\")\n",
    "# bi_df   = parse_contacts_mat(SEEG_BI,   preferred_key=\"sEEG_3D_BIPOLARcontacts_locs\")\n",
    "# mono_pts = mono_df[[\"x\",\"y\",\"z\"]].to_numpy()\n",
    "# bi_pts   = bi_df[[\"x\",\"y\",\"z\"]].to_numpy()\n",
    "\n",
    "# --- 3) Scelgo quali coordinate dei contatti usare (allineate se presenti) ---\n",
    "def pick_contacts_for_control(mono_pts, bi_pts):\n",
    "    \"\"\"\n",
    "    Restituisce (mono_use, bi_use):\n",
    "    - Se esistono CSV allineati_to_B0 -> li carica e li usa\n",
    "    - Altrimenti usa i punti in memoria (mono_pts/bi_pts)\n",
    "    \"\"\"\n",
    "    # MONOPOLARI\n",
    "    if MONO_ALIGNED_CSV.exists():\n",
    "        df = pd.read_csv(MONO_ALIGNED_CSV)\n",
    "        mono_use = df[[\"x\",\"y\",\"z\"]].to_numpy()\n",
    "        print(f\"[control set] Uso MONO allineati: {MONO_ALIGNED_CSV.name}\")\n",
    "    else:\n",
    "        mono_use = mono_pts\n",
    "        print(\"[control set] ATTENZIONE: non trovo MONO allineati; uso i MONO correnti.\")\n",
    "    # BIPOLARI\n",
    "    if BI_ALIGNED_CSV.exists():\n",
    "        df = pd.read_csv(BI_ALIGNED_CSV)\n",
    "        bi_use = df[[\"x\",\"y\",\"z\"]].to_numpy()\n",
    "        print(f\"[control set] Uso BI allineati: {BI_ALIGNED_CSV.name}\")\n",
    "    else:\n",
    "        bi_use = bi_pts\n",
    "        print(\"[control set] ATTENZIONE: non trovo BI allineati; uso i BI correnti.\")\n",
    "    return mono_use, bi_use\n",
    "\n",
    "# Si assume che 'mono_pts' e 'bi_pts' siano in memoria dal blocco precedente\n",
    "mono_for_ctrl, bi_for_ctrl = pick_contacts_for_control(mono_pts, bi_pts)\n",
    "\n",
    "# --- 4) Funzione: costruzione della tabella contatto → ROI k-NN (con soglia MAX_DIST) ---\n",
    "def build_contact_to_roi(points: np.ndarray,\n",
    "                         contact_labels: list[str] | None,\n",
    "                         roi_xyz: np.ndarray,\n",
    "                         roi_ids: np.ndarray,\n",
    "                         k: int = 1,\n",
    "                         max_dist: float | None = None,\n",
    "                         tag: str = \"type\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    la funzione calcola i k centroidi B0 più vicini a ciascun contatto sEEG (knn_indeces_dists)\n",
    "    points:      (N,3) contatti (idealmente in B0)\n",
    "    contact_labels: lista etichette (se None, creo C1..CN)\n",
    "    roi_xyz:     (M,3) centroidi B0\n",
    "    roi_ids:     (M,)  ID ROI coerenti con connettoma\n",
    "    k:           numero di vicini da tenere\n",
    "    max_dist:    soglia distanza per filtrare coppie troppo lontane (None = nessun filtro)\n",
    "    tag:         \"monopolar\" / \"bipolar\" per etichettare le righe\n",
    "    \"\"\"\n",
    "    if points is None or points.size == 0:\n",
    "        return pd.DataFrame(columns=[\"type\",\"contact_index\",\"contact_label\",\"roi_id\",\"roi_rank\",\"distance\"])\n",
    "    # etichette contatto: se non fornite, creo C1..CN\n",
    "    if (contact_labels is None) or (len(contact_labels) != points.shape[0]):\n",
    "        contact_labels = [f\"C{i+1}\" for i in range(points.shape[0])]\n",
    "\n",
    "    # k-nearest neighbors\n",
    "    d, idx = knn_indices_dists(points, roi_xyz, k=k)  # shape (N,k) ciascuno\n",
    "\n",
    "    rows = []\n",
    "    for i in range(points.shape[0]):\n",
    "        for r in range(k):\n",
    "            dist_ir = float(d[i, r])\n",
    "            if (max_dist is not None) and (dist_ir > max_dist):\n",
    "                continue\n",
    "            roi_id = int(roi_ids[idx[i, r]])  # ID coerente col connettoma\n",
    "            rows.append({\n",
    "                \"type\": tag,\n",
    "                \"contact_index\": int(i+1),           # 1-based per leggibilità\n",
    "                \"contact_label\": contact_labels[i],  # es. A'3, B'7, ecc.\n",
    "                \"roi_id\": roi_id,\n",
    "                \"roi_rank\": int(r+1),                # 1=più vicina\n",
    "                \"distance\": dist_ir\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- 5) Etichette dei contatti (se i DataFrame dai .mat esistono) ---\n",
    "mono_labels = mono_df[\"label\"].tolist() if 'mono_df' in globals() else None\n",
    "bi_labels   = bi_df[\"label\"].tolist()   if 'bi_df'   in globals() else None\n",
    "\n",
    "# --- 5) Costruisco le mappe per monopolar e bipolar ---\n",
    "df_map_mono = build_contact_to_roi(\n",
    "    points=mono_for_ctrl,\n",
    "    contact_labels=mono_labels,\n",
    "    roi_xyz=b0_xyz,\n",
    "    roi_ids=b0_ids,\n",
    "    k=K_NEAREST,\n",
    "    max_dist=MAX_DIST,\n",
    "    tag=\"monopolar\"\n",
    ")\n",
    "df_map_bi = build_contact_to_roi(\n",
    "    points=bi_for_ctrl,\n",
    "    contact_labels=bi_labels,\n",
    "    roi_xyz=b0_xyz,\n",
    "    roi_ids=b0_ids,\n",
    "    k=K_NEAREST,\n",
    "    max_dist=MAX_DIST,\n",
    "    tag=\"bipolar\"\n",
    ")\n",
    "\n",
    "# --- 6) Salvo le mappe complete (k=1 -> 1 riga per contatto) ---\n",
    "map_mono_path = OUT / \"controlset_map_monopolar_k1.csv\"\n",
    "map_bi_path   = OUT / \"controlset_map_bipolar_k1.csv\"\n",
    "df_map_mono.to_csv(map_mono_path, index=False)\n",
    "df_map_bi.to_csv(map_bi_path,   index=False)\n",
    "\"ogni riga = 1 contatto (mono o bi); \"\n",
    "\"colonne: type, contact_index, contact_label, roi_id, roi_rank, distance (mm)\"\n",
    "\n",
    "# --- 7) Estraggo la top-1 per contatto ---\n",
    "mono_top1 = (df_map_mono\n",
    "             .sort_values([\"contact_index\",\"roi_rank\"])\n",
    "             .groupby(\"contact_index\", as_index=False)\n",
    "             .first())\n",
    "\n",
    "# (opzionale) audit anche per bipolari\n",
    "bi_top1   = (df_map_bi\n",
    "             .sort_values([\"contact_index\",\"roi_rank\"])\n",
    "             .groupby(\"contact_index\", as_index=False)\n",
    "             .first())\n",
    "\"qui per sicurezza faccio un groupby(contact_index) e tengo solo la riga con roi_rank più piccolo.\"\n",
    "\"In teoria con k=1 non serve, ma se in futuro cambio k > 1, è utile e robusto.\"\n",
    "\n",
    "# Salvo le mappe per-contatto (comode per QC)\n",
    "mono_top1_path = OUT / \"contact2roi_MONO_top1.csv\"\n",
    "bi_top1_path   = OUT / \"contact2roi_BI_top1.csv\"\n",
    "mono_top1.to_csv(mono_top1_path, index=False)\n",
    "bi_top1.to_csv(bi_top1_path,     index=False)\n",
    "\n",
    "# --- 8) costruzione CONTROL SET PER-CONTATTO (CON DUPLICATI), 1 riga per contatto ---\n",
    "# Ordine dei contatti dato da contact_index; una riga per contatto\n",
    "# estraggo roi_id e ottengo una lista di ID ROI (con duplicati, se più contatti mappano alla stessa ROI)\n",
    "mono_ids_per_contact = mono_top1.sort_values(\"contact_index\")[\"roi_id\"].astype(int).tolist()\n",
    "bi_ids_per_contact   = bi_top1.sort_values(\"contact_index\")[\"roi_id\"].astype(int).tolist()\n",
    "\n",
    "def write_ids_txt(path, ids):\n",
    "    with open(path, \"w\") as f:\n",
    "        for rid in ids:\n",
    "            f.write(f\"{rid}\\n\")\n",
    "\n",
    "ctl_mono_percontact_path = OUT / \"control_set_ROIs_B0_MONO_top1_perContact.txt\"\n",
    "ctl_bi_percontact_path   = OUT / \"control_set_ROIs_B0_BI_top1_perContact.txt\"\n",
    "write_ids_txt(ctl_mono_percontact_path, mono_ids_per_contact)\n",
    "write_ids_txt(ctl_bi_percontact_path,   bi_ids_per_contact)\n",
    "\"contengono:\"\n",
    "\"ogni riga = ID di 1 ROI (intero da 1 a 850) associata al contatto sEEG più vicino corrispondente.\"\n",
    "\"il numero di righe corrisponde al n° di contatti sEEG considerati:\"\n",
    "\"- MONO: attesi 86 contatti (righe)\"\n",
    "\"- BI:   attesi 77 contatti (righe)\"\n",
    "\n",
    "# --- Check: devo avere 86 righe nel MONO (una per contatto) ---\n",
    "n_contacts_mono = mono_top1[\"contact_index\"].nunique()\n",
    "print(f\"[CHECK] MONO per-contatto: {len(mono_ids_per_contact)} righe (contatti unici={n_contacts_mono})\")\n",
    "if len(mono_ids_per_contact) != 86:\n",
    "    print(\"[WARNING] Attese 86 righe. Se sono meno, verifica MAX_DIST (deve essere None) o contatti inattivi.\")\n",
    "\n",
    "# --- Check: devo avere 77 righe nel BI (una per contatto) ---\n",
    "n_contacts_bi = bi_top1[\"contact_index\"].nunique()\n",
    "print(f\"[CHECK] BI per-contatto: {len(bi_ids_per_contact)} righe (contatti unici={n_contacts_bi})\")\n",
    "if len(bi_ids_per_contact) != 77:\n",
    "    print(\"[WARNING] Attese 77 righe. Se sono meno, verifica MAX_DIST (deve essere None) o contatti inattivi.\")\n",
    "\n",
    "print(\"\\n== MAPPE & CONTROL SET (PER-CONTATTO, CON DUPLICATI) COSTRUITI ==\")\n",
    "print(f\"- Mappe complete:      {map_mono_path.name} | {map_bi_path.name}\")\n",
    "print(f\"- Mappe top-1:         {mono_top1_path.name} | {bi_top1_path.name}\")\n",
    "print(f\"- Control set MONO PC: {ctl_mono_percontact_path.name} (#righe={len(mono_ids_per_contact)}, contatti unici={n_contacts_mono})\")\n",
    "print(f\"- Control set BI   PC: {ctl_bi_percontact_path.name}   (#righe={len(bi_ids_per_contact)}, contatti unici={n_contacts_bi})\")\n",
    "if len(mono_ids_per_contact) != 86:\n",
    "    print(\"[WARNING] Attese 86 righe per i MONO. Se sono meno, verifica che MAX_DIST=None e i contatti disponibili.\")\n",
    "print(\">> Per la control energy usa:\", ctl_mono_percontact_path.name)\n",
    "\n",
    "# Nota importante:\n",
    "# Gli ID salvati nel control set (uno per riga) devono corrispondere all'ordine delle ROI nel mio connettoma\n",
    "# (connectome850_2b0.xlsx). Verificare che \"b0_ids\" sia la stessa indicizzazione usata per la matrice A.\n",
    "\n",
    "# Risultati:\n",
    "# file output/control_set_ROIs_B0_*.txt contiene la lista di ID ROI da usare come nodi di controllo (matrice B nella NCT).\n",
    "# I CSV controlset_map_*.csv permettono di ispezionare quali ROI sono state associate ad ogni contatto (utile per QC o per cambiare k/soglia).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f4372557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# QC distanza (ROI più vicina per contatto)\n",
      "GLOBAL: {'N': 163, 'median': 3.1179013495002184, 'mean': 4.3606404403643655, 'p75': 5.334933307124771, 'p90': 9.054064650602902, 'p95': 11.193904067730934, 'max': 15.289535775049597}\n",
      "BIPOLAR: {'N': 77, 'median': 3.1889018017358635, 'mean': 4.461605892017394, 'p75': 5.338793143566348, 'p90': 9.03706437112547, 'p95': 10.960992178744535, 'max': 15.289535775049597}\n",
      "MONOPOLAR: {'N': 86, 'median': 3.005098189196934, 'mean': 4.2702411406285155, 'p75': 5.3095441252747815, 'p90': 9.00682530492946, 'p95': 11.076415912739773, 'max': 14.481755895596129}\n",
      "\n",
      "Frazione di contatti con ROI più vicina entro 6.0 mm (GLOBAL): 77.9%\n",
      "Outlier globali (> 6.0 mm) salvati in: c:\\Users\\barba\\Visual Studio - Github\\NCT\\output\\controlset_top1_outliers_gt_6.0mm.csv\n",
      "[QC] bipolar top-1 → {'N': 77, 'median': 3.1889018017358635, 'mean': 4.461605892017394, 'p90': 9.03706437112547, 'max': 15.289535775049597} ; quota ≤ 6.0 mm = 76.6%\n",
      "[QC]  Outlier bipolar salvati in: c:\\Users\\barba\\Visual Studio - Github\\NCT\\output\\bipolar_top1_outliers_gt_6.0mm.csv\n",
      "[QC] monopolar top-1 → {'N': 86, 'median': 3.005098189196934, 'mean': 4.2702411406285155, 'p90': 9.00682530492946, 'max': 14.481755895596129} ; quota ≤ 6.0 mm = 79.1%\n",
      "[QC]  Outlier monopolar salvati in: c:\\Users\\barba\\Visual Studio - Github\\NCT\\output\\monopolar_top1_outliers_gt_6.0mm.csv\n",
      "\n",
      "[QC] Midpoint bipolari → median dist=0.000 mm ; median ratio=0.000\n",
      "[QC] Bipolari con dist_to_midpoint > 2.0 mm: 0\n",
      "\n",
      "[QC] Control set (MONO per-contatto): control_set_ROIs_B0_MONO_top1_perContact.txt → #righe=86 ; #unici=57 ; duplicati=29\n",
      "[QC]  Tutti gli ID presenti in B0 ✅\n",
      "[QC]  Range ID compatibile con connectome (N=850) ✅\n",
      "\n",
      "Creato bundle Excel QC: c:\\Users\\barba\\Visual Studio - Github\\NCT\\output\\nct_outputs_bundle.xlsx\n",
      "[QC] Fine QC completo ✅\n"
     ]
    }
   ],
   "source": [
    "# ============ QC COMPLETO CONTROL SET: distanze, midpoint, integrità + crea bundle Excel =============\n",
    "# il seguente blocco calcola statistiche globali e per tipo con percentili, \n",
    "# calcola e salva outlier globali e outlier per tipo,\n",
    "# crea un istogramma delle distanze,\n",
    "# esegue un QC sui midpoint dei bipolari,\n",
    "# controlla che il control set MONO per-contatto abbia 86 righe, sia compatibile con B0 e col connettoma\n",
    "# crea il file Excel unico con tutti i .csv importanti\n",
    "\n",
    "# Parametri-soglia\n",
    "THR_TOP1_MM = 6.0       # (mm) soglia distanza contatto - ROI\n",
    "THR_MIDPOINT_MM = 2.0   # (mm) soglia distanza bipolare - midpoint\n",
    "\n",
    "# Carico mappe mono/bi\n",
    "map_mono_path = OUT / \"controlset_map_monopolar_k1.csv\"\n",
    "map_bi_path   = OUT / \"controlset_map_bipolar_k1.csv\"\n",
    "\n",
    "# --- Helper: carica mappa e restituisce solo la ROI top-1 (roi_rank==1 se esiste) ---\n",
    "# una sola funzione per leggere le mappe e tenere solo roi_rank==1\n",
    "def load_top1_df(path: Path, label: str) -> pd.DataFrame | None:\n",
    "    if not path.exists():\n",
    "        print(f\"[QC] {label}: file non trovato ({path.name})\")\n",
    "        return None\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    if \"roi_rank\" in df.columns:\n",
    "        top1 = df[df[\"roi_rank\"] == 1].copy()\n",
    "        if top1.empty:\n",
    "            print(f\"[QC] {label}: nessuna riga roi_rank==1 (uso l'intero df)\")\n",
    "            top1 = df.copy()\n",
    "    else:\n",
    "        top1 = df.copy()\n",
    "\n",
    "    top1[\"type\"] = label\n",
    "    return top1\n",
    "\n",
    "def stats_dict(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return {\n",
    "        \"N\": int(x.size),\n",
    "        \"median\": float(np.median(x)),\n",
    "        \"mean\": float(np.mean(x)),\n",
    "        \"p75\": float(np.percentile(x, 75)),\n",
    "        \"p90\": float(np.percentile(x, 90)),\n",
    "        \"p95\": float(np.percentile(x, 95)),\n",
    "        \"max\": float(np.max(x)),\n",
    "    }\n",
    "\n",
    "# --- 1) QC distanze contatto–ROI (mono + bi) ---\n",
    "top1_m = load_top1_df(map_mono_path, \"monopolar\")\n",
    "top1_b = load_top1_df(map_bi_path,   \"bipolar\")\n",
    "frames = [d for d in (top1_m, top1_b) if d is not None]\n",
    "\n",
    "if frames:\n",
    "    top1 = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    print(\"\\n# QC distanza (ROI più vicina per contatto)\")\n",
    "    print(\"GLOBAL:\", stats_dict(top1[\"distance\"].to_numpy()))\n",
    "    for t_lab, g in top1.groupby(\"type\"):\n",
    "        print(f\"{t_lab.upper()}:\", stats_dict(g[\"distance\"].to_numpy()))\n",
    "\n",
    "    # quota entro soglia THR_TOP1_MM (globale)\n",
    "    frac_within = (top1[\"distance\"] <= THR_TOP1_MM).mean()\n",
    "    print(f\"\\nFrazione di contatti con ROI più vicina entro {THR_TOP1_MM} mm (GLOBAL): {frac_within:.1%}\")\n",
    "\n",
    "    # outlier globali\n",
    "    outliers_global = top1[top1[\"distance\"] > THR_TOP1_MM].sort_values(\"distance\", ascending=False)\n",
    "    out_path_global = OUT / f\"controlset_top1_outliers_gt_{THR_TOP1_MM}mm.csv\"\n",
    "    outliers_global.to_csv(out_path_global, index=False)\n",
    "    print(f\"Outlier globali (> {THR_TOP1_MM} mm) salvati in: {out_path_global}\")\n",
    "\n",
    "    # outlier per tipo (come nel QC light)\n",
    "    for t_lab, g in top1.groupby(\"type\"):\n",
    "        arr = g[\"distance\"].to_numpy(dtype=float)\n",
    "        stats = dict(\n",
    "            N=int(arr.size),\n",
    "            median=float(np.median(arr)),\n",
    "            mean=float(np.mean(arr)),\n",
    "            p90=float(np.percentile(arr, 90)),\n",
    "            max=float(np.max(arr)),\n",
    "        )\n",
    "        frac_t = float((arr <= THR_TOP1_MM).mean())\n",
    "        print(f\"[QC] {t_lab} top-1 → {stats} ; quota ≤ {THR_TOP1_MM} mm = {frac_t:.1%}\")\n",
    "\n",
    "        outliers_t = g[g[\"distance\"] > THR_TOP1_MM].sort_values(\"distance\", ascending=False)\n",
    "        if not outliers_t.empty:\n",
    "            outp = OUT / f\"{t_lab}_top1_outliers_gt_{THR_TOP1_MM}mm.csv\"\n",
    "            outliers_t.to_csv(outp, index=False)\n",
    "            print(f\"[QC]  Outlier {t_lab} salvati in: {outp}\")\n",
    "\n",
    "    # istogramma globale distanze\n",
    "    plt.figure()\n",
    "    plt.hist(top1[\"distance\"], bins=20)\n",
    "    plt.title(\"Distanza ROI più vicina (top-1)\")\n",
    "    plt.xlabel(\"Distanza euclidea [mm]\")\n",
    "    plt.ylabel(\"Conteggio contatti\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT / \"distance_nearest_ROI.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"[QC] ATTENZIONE: nessuna mappa mono/bi trovata. Salto QC distanze.\")\n",
    "\n",
    "# --- 2) QC midpoint bipolari ---\n",
    "mid_path = OUT / \"check_bipolar_midpoint.csv\"\n",
    "if mid_path.exists():\n",
    "    mid = pd.read_csv(mid_path)\n",
    "    med_dist  = float(np.median(mid[\"dist_to_midpoint\"]))\n",
    "    med_ratio = float(np.median(mid[\"midpoint_ratio\"]))\n",
    "    print(f\"\\n[QC] Midpoint bipolari → median dist={med_dist:.3f} mm ; median ratio={med_ratio:.3f}\")\n",
    "    bad_mid = mid[mid[\"dist_to_midpoint\"] > THR_MIDPOINT_MM]\n",
    "    print(f\"[QC] Bipolari con dist_to_midpoint > {THR_MIDPOINT_MM} mm: {len(bad_mid)}\")\n",
    "    if not bad_mid.empty:\n",
    "        outp = OUT / f\"midpoint_outliers_gt_{THR_MIDPOINT_MM}mm.csv\"\n",
    "        bad_mid.to_csv(outp, index=False)\n",
    "        print(f\"[QC]  Outlier midpoint salvati in: {outp}\")\n",
    "else:\n",
    "    print(\"\\n[QC] ATTENZIONE: check_bipolar_midpoint.csv non trovato (salta questo check).\")\n",
    "\n",
    "# --- 3) Integrità control set MONO per-contatto ---\n",
    "ctrl_path = OUT / \"control_set_ROIs_B0_MONO_top1_perContact.txt\"\n",
    "if ctrl_path.exists():\n",
    "    ctrl_ids = np.loadtxt(ctrl_path, dtype=int, ndmin=1)  # mantiene ordine e duplicati\n",
    "    n_total  = int(ctrl_ids.size)\n",
    "    n_unique = len(set(ctrl_ids.tolist()))\n",
    "    n_dupes  = n_total - n_unique\n",
    "    print(f\"\\n[QC] Control set (MONO per-contatto): {ctrl_path.name} → #righe={n_total} ; #unici={n_unique} ; duplicati={n_dupes}\")\n",
    "    if n_total != 86:\n",
    "        print(\"[QC]  WARNING: attese 86 righe. Se sono meno, verifica MAX_DIST=None o contatti mancanti.\")\n",
    "\n",
    "    # Verifica presenza in B0\n",
    "    b0_df  = pd.read_csv(B0_TXT, sep=r\"[\\s,;]+\", engine=\"python\",\n",
    "                         header=None, comment=\"#\", skip_blank_lines=True)\n",
    "    b0_ids = b0_df.iloc[:, 0].astype(int).to_numpy()\n",
    "    missing = sorted(set(ctrl_ids) - set(b0_ids))\n",
    "    if len(missing) == 0:\n",
    "        print(\"[QC]  Tutti gli ID presenti in B0 ✅\")\n",
    "    else:\n",
    "        print(f\"[QC]  ATTENZIONE: ID non presenti in B0: {missing[:10]}{' ...' if len(missing) > 10 else ''}\")\n",
    "\n",
    "    # Coerenza con la dimensione del connectome\n",
    "    conn = BASE / \"connectome850_2b0.xlsx\"\n",
    "    if conn.exists():\n",
    "        try:\n",
    "            A_tmp = pd.read_excel(conn, header=None)\n",
    "            nA = A_tmp.shape[0]\n",
    "            if A_tmp.shape[0] != A_tmp.shape[1]:\n",
    "                print(f\"[QC]  ATTENZIONE: connectome non quadrato: shape={A_tmp.shape}\")\n",
    "            bad = [int(i) for i in ctrl_ids if not (1 <= i <= nA)]\n",
    "            if bad:\n",
    "                print(f\"[QC]  ATTENZIONE: ID fuori range [1..{nA}]: {bad[:10]}{' ...' if len(bad) > 10 else ''}\")\n",
    "            else:\n",
    "                print(f\"[QC]  Range ID compatibile con connectome (N={nA}) ✅\")\n",
    "        except Exception as e:\n",
    "            print(f\"[QC]  Nota: non riesco a leggere {conn.name} ({e})\")\n",
    "else:\n",
    "    print(\"\\n[QC] ATTENZIONE: control_set_ROIs_B0_MONO_top1_perContact.txt non trovato.\")\n",
    "\n",
    "# --- 4) Bundle Excel con output principali (come nel blocco originale) ---\n",
    "with pd.ExcelWriter(OUT / \"nct_outputs_bundle.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for p, name in [\n",
    "        (OUT/\"sEEG_monopolar_aligned_to_MNI.csv\", \"mono_MNI\"),\n",
    "        (OUT/\"sEEG_bipolar_aligned_to_MNI.csv\",   \"bi_MNI\"),\n",
    "        (OUT/\"sEEG_monopolar_aligned_to_B0.csv\",  \"mono_B0\"),\n",
    "        (OUT/\"sEEG_bipolar_aligned_to_B0.csv\",    \"bi_B0\"),\n",
    "        (OUT/\"bbox_summary.csv\",                  \"bbox\"),\n",
    "        (OUT/\"check_bipolar_midpoint.csv\",        \"midpoint_QC\"),\n",
    "    ]:\n",
    "        if p.exists():\n",
    "            pd.read_csv(p).to_excel(writer, sheet_name=name, index=False)\n",
    "print(\"\\nCreato bundle Excel QC:\", OUT / \"nct_outputs_bundle.xlsx\")\n",
    "print(\"[QC] Fine QC completo ✅\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b64efd",
   "metadata": {},
   "source": [
    "CONTROL ENERGY - PROCEDURE 1_paper Parkes et al. -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "712f505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Control set (BI per-contatto): letti 77 indici da control_set_ROIs_B0_BI_top1_perContact.txt\n",
      "[CHECK] Connettoma caricato: shape=(850, 850)\n",
      "[CHECK] Parcellazione = 850 ✅\n",
      "[CHECK] Diagonale con 833 valori ≠0 → azzero.\n"
     ]
    }
   ],
   "source": [
    "# Control Energy (Procedure 1 Parkes et al.) adattata ai miei dati:\n",
    "\"\"\"\n",
    "Adattamento minimo della Procedure 1 (Parkes et al., 2024) al mio setup:\n",
    "- A (adjacent matrix) da data/connectome850_2b0.xlsx (NxN)\n",
    "- B (control set) da output/control_set_ROIs_B0_BI_top1_perContact.txt (indici 1-based)\n",
    "- Stati: Vis -> Default (se labels disponibili) altrimenti fallback KMeans\n",
    "\"\"\"\n",
    "# - Stati:      iniziale = ROI MONO del mio control set ; target = complemento\n",
    "# - B (controllo): partial control (1 sui miei nodi, 1e-3 sugli altri)\n",
    "# - Sistema:    continuous-time, normalizzazione c=1\n",
    "# Salva figure e risultati in: output/control_energy/ \n",
    "# -----------------------------------------------------------\n",
    "\n",
    "CTL_TXT   = OUT / \"control_set_ROIs_B0_BI_top1_perContact.txt\"   #  control set\n",
    "CONN_XLSX = BASE / \"connectome850_2b0.xlsx\"                    #  connettoma strutturale\n",
    "\n",
    "# -- Caricamento CONTROL SET (lista di indici 1-based, con duplicati e ordinati) --\n",
    "try:\n",
    "    ctl_ids = np.loadtxt(CTL_TXT, dtype=int, ndmin=1)  # mantiene duplicati e ordine\n",
    "except Exception:\n",
    "    # fallback robusto se il file avesse righe/spazi strani\n",
    "    ctl_ids = []\n",
    "    with open(CTL_TXT, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                ctl_ids.append(int(float(line)))\n",
    "    ctl_ids = np.array(ctl_ids, dtype=int)\n",
    "\n",
    "print(f\"[CHECK] Control set (BI per-contatto): letti {ctl_ids.size} indici da {CTL_TXT.name}\")\n",
    "if ctl_ids.size != 77:\n",
    "    print(\"[WARNING] Il control set non ha 77 righe. Controlla l’estrazione per-contatto e MAX_DIST=None.\")\n",
    "\n",
    "# -- Caricamento CONNETTOMA da Excel→ numpy, fix semplice se non quadrato --\n",
    "#   - header=None per evitare che la prima riga diventi header\n",
    "#   - coerce numeric, drop righe/colonne completamente NaN\n",
    "# Next, we load a structural connectome as our adjacency matrix:\n",
    "# # directory where data is stored \n",
    "# datadir = '/path/to/data'   \n",
    "# adjacency_file = 'structural_connectome.npy' \n",
    "# # load adjacency matrix \n",
    "# adjacency = np.load(os.path.join(datadir, adjacency_file))\n",
    "\n",
    "def load_adjacency_from_excel(path: Path) -> np.ndarray:\n",
    "    df = pd.read_excel(path, header=None)\n",
    "    # Forza numerico, elimina colonne/righe completamente vuote\n",
    "    for c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "    A = df.to_numpy(dtype=float)\n",
    "\n",
    "    # Se non quadrata, tenta una correzione semplice (drop prima riga/col)\n",
    "    if A.shape[0] != A.shape[1]:\n",
    "        # prova drop col0\n",
    "        if A.shape[1] - 1 == A.shape[0]:\n",
    "            A = A[:, 1:]\n",
    "        # prova drop row0\n",
    "        elif A.shape[0] - 1 == A.shape[1]:\n",
    "            A = A[1:, :]\n",
    "        # prova drop row0 e col0\n",
    "        elif A.shape[0] - 1 == A.shape[1] - 1:\n",
    "            A = A[1:, 1:]\n",
    "    assert A.shape[0] == A.shape[1], f\"Matrice non quadrata anche dopo fix: {A.shape}\"\n",
    "    # Sostituisci eventuali NaN con 0\n",
    "    A = np.nan_to_num(A, copy=False)\n",
    "    return A\n",
    "\n",
    "A = load_adjacency_from_excel(CONN_XLSX)\n",
    "N = A.shape[0]  #n_nodes\n",
    "print(f\"[CHECK] Connettoma caricato: shape={A.shape}\")\n",
    "# -- PARCELLATION: 800 vs 850 e coerenza con control set --\n",
    "if N == 850:\n",
    "    print(\"[CHECK] Parcellazione = 850 ✅\")\n",
    "elif N == 800:\n",
    "    print(\"[CHECK] Parcellazione = 800 ⚠️ verifica control set\")\n",
    "else:\n",
    "    print(f\"[CHECK] Parcellazione non standard (N={N}) ⚠️\")\n",
    "# range e duplicati nel control set\n",
    "if ctl_ids.min() < 1 or ctl_ids.max() > N:\n",
    "    raise ValueError(f\"[CHECK] ID control set fuori range 1..{N} (min={ctl_ids.min()}, max={ctl_ids.max()}).\")\n",
    "\n",
    "# pulizia A: simmetria e diagonale\n",
    "if not np.isfinite(A).all():\n",
    "    raise ValueError(\"[CHECK] A contiene NaN/±inf.\")\n",
    "# simmetria (per reti non dirette ci si aspetta A≈A^T)\n",
    "asym = np.linalg.norm(A - A.T, ord='fro') / max(1.0, np.linalg.norm(A, ord='fro'))\n",
    "if asym > 1e-6:\n",
    "    print(f\"[CHECK] A non perfettamente simmetrica (relFro={asym:.2e}) → simmetrizzo.\")\n",
    "    A = (A + A.T) / 2.0\n",
    "# diagonale: molte pipeline NCT usano diag=0 (niente self-loops)\n",
    "n_diag_nz = int(np.sum(np.abs(np.diag(A)) > 1e-12))\n",
    "if n_diag_nz > 0:\n",
    "    print(f\"[CHECK] Diagonale con {n_diag_nz} valori ≠0 → azzero.\")\n",
    "    np.fill_diagonal(A, 0.0)\n",
    "\n",
    "# -- Salvataggi “puliti” e report sintetico --\n",
    "pd.DataFrame(A).to_csv(OUT / \"A_struct_clean.csv\", index=False, header=False)\n",
    "pd.DataFrame([{\n",
    "    \"N\": N,\n",
    "    \"control_set_size\": int(ctl_ids.size),\n",
    "    \"control_set_min\": int(ctl_ids.min()),\n",
    "    \"control_set_max\": int(ctl_ids.max()),\n",
    "    \"diag_nonzero_before\": n_diag_nz,\n",
    "    \"asymmetry_relFro\": float(asym),\n",
    "}]).to_csv(OUT / \"control_energy_qc_summary.csv\", index=False)\n",
    "\n",
    "# -- Preparazione vettore b e matrice B per la NCT --\n",
    "# b: vettore di controllo (1 se ROI controllata, 0 altrimenti), 1-based → 0-based\n",
    "b = np.zeros(N, dtype=float)\n",
    "b[ctl_ids - 1] = 1.0    # le righe duplicate non cambiano il risultato (assegnazioni ripetute a 1)\n",
    "pd.DataFrame(b, columns=[\"b\"]).to_csv(OUT / \"B_vector_diag.csv\", index=False)\n",
    "# matrice B diag (NxN) se si preferisce lo stile B=diag(b)\n",
    "B = np.diag(b)\n",
    "pd.DataFrame(B).to_csv(OUT/'B_diag_matrix.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0949093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_norm pronto (system=continuous, c=1).\n",
      "[STATE] #ROI controllate (da BI per-contatto, deduplicate) = 50 / 850\n",
      "  initial mean strength: 17071.86\n",
      "  target  mean strength: 16975.72\n",
      "x_interictal shape: (850,)\n",
      "x_ictal      shape: (850,)\n",
      "[STATE] Stati iniziale e target presi da PSD sEEG (non-binari, normalizzati).\n",
      "inversion error = 1.49E-10 (<1.00E-08=True)\n",
      "reconstruction error = 3.12E-09 (<1.00E-08=True)\n",
      "node_energy shape: (850,)\n",
      "[SAVE] node_energy_850.npy salvato in c:\\Users\\barba\\Visual Studio - Github\\NCT\\output\n",
      "Total control energy: 35.9815\n",
      "[DONE] Risultati scritti in: c:\\Users\\barba\\Visual Studio - Github\\NCT\\output\\control_energy\n"
     ]
    }
   ],
   "source": [
    "# ============================ Time System — passo 1 =========================================\n",
    "# 1. Define a time system. Determine whether to model the linear dynamical system in discrete \n",
    "# or continuous time:\n",
    "# system = \"discrete\" # or system = \"continuous\"\n",
    "# --- come nel paper: continuo ---\n",
    "system = \"continuous\"\n",
    "\n",
    "# ============================ Normalizzazione di A — passo 2 =========================================\n",
    "# 2. Normalize the adjacency matrix. Once a time system has been determined, normalize the \n",
    "# adjacency matrix, A:\n",
    "# adjacency_norm = matrix_normalization(A=adjacency, system=system, c=1)\n",
    "A_norm = matrix_normalization(A, system=system, c=1)\n",
    "print(\"A_norm pronto (system=continuous, c=1).\")\n",
    "\n",
    "# ========================= Definizione degli stati del control task — passo 3 ============================\n",
    "# 3. Definizione degli stati iniziale e target (x0, xf) dal control set\n",
    "def load_roi_list_1based_to_mask(txt_path: Path, n_nodes: int) -> np.ndarray:\n",
    "    ids = []\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.replace(\",\", \" \").split()\n",
    "            if parts:\n",
    "                try:\n",
    "                    idx1 = int(float(parts[0]))\n",
    "                    i0 = idx1 - 1\n",
    "                    if 0 <= i0 < n_nodes:\n",
    "                        ids.append(i0)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    ids = sorted(set(ids))\n",
    "    mask = np.zeros(n_nodes, dtype=bool)\n",
    "    mask[ids] = True\n",
    "    return mask\n",
    "\n",
    "initial_mask = load_roi_list_1based_to_mask(CTL_TXT, N) # 77 ROI da BI per-contatto\n",
    "n_ctrl = int(initial_mask.sum())\n",
    "print(f\"[STATE] #ROI controllate (da BI per-contatto, deduplicate) = {n_ctrl} / {N}\")\n",
    "\n",
    "target_mask  = ~initial_mask    # è solo una mask per distinguere nodi controllati vs non controllati\n",
    "                                # (utile per B e per i plot)\n",
    "strength = A.sum(axis=0)    # Strength (diagnostica)\n",
    "print(\"  initial mean strength: {:.2f}\".format(np.mean(strength[initial_mask])))\n",
    "print(\"  target  mean strength: {:.2f}\".format(np.mean(strength[target_mask])))\n",
    "\n",
    "# Caricamento da MATLAB degli stati x0 = x_interictal & xf = x_ictal\n",
    "STATE_MATE = BASE / \"states_NCT_from_sEEG.mat\"\n",
    "mat_states = loadmat(STATE_MATE)\n",
    "\n",
    "x_interictal = np.asarray(mat_states[\"x_interictal_80_150\"], dtype=float).ravel()\n",
    "x_ictal      = np.asarray(mat_states[\"x_ictal_80_150\"],      dtype=float).ravel()\n",
    "\n",
    "print(\"x_interictal shape:\", x_interictal.shape)\n",
    "print(\"x_ictal      shape:\", x_ictal.shape)\n",
    "\n",
    "assert x_interictal.shape[0] == N == A.shape[0]\n",
    "assert x_ictal.shape[0] == N == A.shape[0]\n",
    "\n",
    "# --- Normalizzazione come nel paper: stato di norma unitaria ---\n",
    "initial_state_raw = x_interictal # stato iniziale grezzo (intercritico, baseline)\n",
    "target_state_raw  = x_ictal      # stato target grezzo (ictale)\n",
    "\n",
    "initial_state = normalize_state(x_interictal)\n",
    "target_state  = normalize_state(x_ictal)\n",
    "\n",
    "# salvo gli stati usati (opzionale)\n",
    "np.save(OUT / \"initial_state_850_sEEG.npy\", np.asarray(initial_state, dtype=float))\n",
    "np.save(OUT / \"target_state_850_sEEG.npy\",  np.asarray(target_state, dtype=float))\n",
    "print(\"[STATE] Stati iniziale e target presi da PSD sEEG (non-binari, normalizzati).\")\n",
    "\n",
    "\"adesso, initial state = x_interictal normalizzato ; target state = x_ictal normalizzato;\"\n",
    "\"non c'è più alcun complemento: sono proprio i pattern di PSD z-scored normalizzati calcolati in MATLAB\"\n",
    "\n",
    "# == Partial control (1 su miei nodi, 1e-3 sugli altri): con questo scelgo dove applicare i control signals\n",
    "control_set = np.zeros((N, N), dtype=float)\n",
    "control_set[initial_mask, initial_mask] = 1.0 # controllo completo sui nodi iniziali\n",
    "control_set[~initial_mask, ~initial_mask] = 1e-7 # controllo minimo sui nodi non iniziali (mettere a 0)\n",
    "\n",
    "# == Heatmap dell’adjacency riordinata per stato (opzionale)\n",
    "if sns is not None:\n",
    "    order = np.r_[np.where(initial_mask)[0], np.where(~initial_mask)[0]]\n",
    "    f, ax = plt.subplots(figsize=(7, 7))\n",
    "    sns.heatmap(A[order, :][:, order], ax=ax, square=True, cmap=\"inferno\",\n",
    "                cbar_kws={\"label\": \"connection strength\", \"shrink\": 0.8})\n",
    "    ax.set_title(\"Adjacency (ordinata: control-set prima)\")\n",
    "    f.tight_layout()\n",
    "    f.savefig((OUT / \"control_energy\" / \"adjacency_heatmap.png\"), dpi=300,\n",
    "              bbox_inches=\"tight\", pad_inches=0.01)\n",
    "    plt.close(f)\n",
    "\n",
    "# ============================ Calcolo Control Energy — passo 4 ==============================\n",
    "# Calcolo dei control signals u(t) & state trajectory x(t).\n",
    "# parameteri e risoluzione \n",
    "time_horizon = 1             # come nel paper (unità arbitraria)\n",
    "rho = 1                      # optimal control (peso uguale a traiettoria e input)\n",
    "S = np.eye(N)  # S=trajectory_constraints: vincola tutti i nodi (optimal control)\n",
    "\n",
    "# ottengo la state trajectory x(t) & i control signals u(t)\n",
    "state_trajectory, control_signals, numerical_error = get_control_inputs(\n",
    "    A_norm=A_norm,\n",
    "    T=time_horizon,\n",
    "    B=control_set,\n",
    "    x0=initial_state,\n",
    "    xf=target_state,\n",
    "    system=system,\n",
    "    rho=rho,\n",
    "    S=S,\n",
    ")\n",
    "\n",
    "# Check inversion / reconstruction errors\n",
    "thr = 1e-8  #threshold\n",
    "print(\"inversion error = {:.2E} (<{:.2E}={:})\".format(numerical_error[0], thr, numerical_error[0] < thr))\n",
    "print(\"reconstruction error = {:.2E} (<{:.2E}={:})\".format(numerical_error[1], thr, numerical_error[1] < thr))\n",
    "\n",
    "# ============================ Plot x(t) & u(t) — passo 5 ============================\n",
    "# Nota: nctpy ritorna di solito matrici shape (T, N). Il paper usa righe=tempo, colonne=nodi.\n",
    "mask_init = initial_state_raw > np.median(initial_state_raw) # initial = nodi con PSD interictale alta (sopra la mediana del pattern interictale)\n",
    "mask_target = target_state_raw > np.median(target_state_raw) # target = nodi con PSD ictale alta (sopra la mediana del pattern ictale)\n",
    "mask_byst = ~(mask_init | mask_target)  # bystander = tutto il resto\n",
    "# queste maschere servono solo per suddividere l'energia per gruppi funzionali e colorare i plot.\n",
    "\n",
    "T_steps = state_trajectory.shape[0]           # es. 1001\n",
    "t = np.linspace(0, time_horizon, T_steps)     # asse del tempo [0, T]\n",
    "\n",
    "f, ax = plt.subplots(3, 2, figsize=(7, 7))\n",
    "\n",
    "# A | control signals, x0, \"initial-dominant\"\n",
    "if np.any(mask_init):\n",
    "    ax[0, 0].plot(t, control_signals[:, mask_init], linewidth=0.75)\n",
    "ax[0, 0].set_title(\"A | control signals, x0\")\n",
    "\n",
    "# B | neural activity, x0\n",
    "if np.any(mask_init):\n",
    "    ax[0, 1].plot(t, state_trajectory[:, mask_init], linewidth=0.75)\n",
    "ax[0, 1].set_title(\"B | neural activity, x0\")\n",
    "\n",
    "# C | control signals, xf\n",
    "if np.any(mask_target):\n",
    "    ax[1, 0].plot(t, control_signals[:, mask_target], linewidth=0.75)\n",
    "ax[1, 0].set_title(\"C | control signals, xf\")\n",
    "\n",
    "# D | neural activity, xf\n",
    "if np.any(mask_target):\n",
    "    ax[1, 1].plot(t, state_trajectory[:, mask_target], linewidth=0.75)\n",
    "ax[1, 1].set_title(\"D | neural activity, xf\")\n",
    "\n",
    "# E | control signals, bystanders\n",
    "if np.any(mask_byst):\n",
    "    ax[2, 0].plot(t, control_signals[:, mask_byst], linewidth=0.75)\n",
    "ax[2, 0].set_title(\"E | control signals, bystanders\")\n",
    "\n",
    "# F | neural activity, bystanders\n",
    "if np.any(mask_byst):\n",
    "    ax[2, 1].plot(t, state_trajectory[:, mask_byst], linewidth=0.75)\n",
    "ax[2, 1].set_title(\"F | neural activity, bystanders\")\n",
    "\n",
    "for cax in ax.reshape(-1):\n",
    "    cax.set_ylabel(\"activity\")\n",
    "    cax.set_xlabel(\"time (a.u.)\")\n",
    "    cax.set_xlim(t[0], t[-1])\n",
    "    # tick a 0 e T (coerenti con l’asse del tempo esplicito)\n",
    "    cax.set_xticks([t[0], t[-1]])    \n",
    "    cax.set_xticklabels([0, time_horizon])\n",
    "\n",
    "f.tight_layout()\n",
    "(OUT / \"control_energy\").mkdir(exist_ok=True)\n",
    "f.savefig(OUT / \"control_energy\" / \"plot_xu_850.svg\", dpi=600, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "plt.close(f)\n",
    "\n",
    "# ============================ Control energy — passo 6 =====================================\n",
    "# integro i control signals\n",
    "def integrate_u_compat(u, T):\n",
    "    \"\"\"\n",
    "    Compat fallback per integrare l'energia:\n",
    "    restituisce il vettore per-nodo ∫ u(t)^2 dt usando Simpson se disponibile\n",
    "    oppure trapezi come fallback.\n",
    "    u: array shape (T_steps, N)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # prova ad usare nctpy.energies.integrate_u\n",
    "        return integrate_u(u)\n",
    "    except Exception:\n",
    "        # fallback robusto (trapezi): differenze minime con molti step\n",
    "        \n",
    "        dt = T / (u.shape[0] - 1)\n",
    "        return np.trapezoid(u**2, dx=dt, axis=0)\n",
    "\n",
    "# (A) Calcolo node-level control energy integrando i control signals:\n",
    "node_energy = integrate_u_compat(control_signals, time_horizon)  # shape (N,)\n",
    "print(\"node_energy shape:\", node_energy.shape)\n",
    "# salvataggio\n",
    "np.save(OUT / \"node_energy_850.npy\", np.asarray(node_energy, dtype=float))\n",
    "print(\"[SAVE] node_energy_850.npy salvato in\", OUT)\n",
    "\n",
    "# (B) Riassumo nodal energy:\n",
    "energy = float(np.sum(node_energy))\n",
    "print(\"Total control energy:\", round(energy, 4))\n",
    "\n",
    "# ============================ Salvataggi e riepilogo ===================================\n",
    "RESULT = OUT / \"control_energy\"\n",
    "RESULT.mkdir(exist_ok=True)\n",
    "pd.DataFrame({\"node\": np.arange(1, N + 1), \"node_energy\": node_energy}).to_csv(\n",
    "    RESULT / \"node_energy_850.csv\", index=False\n",
    ")\n",
    "with open(RESULT / \"summary.txt\", \"w\") as fsum:\n",
    "    fsum.write(\n",
    "        f\"N={N}\\n\"\n",
    "        f\"energy={energy:.10f}\\n\"\n",
    "        f\"inversion_err={numerical_error[0]:.4e}\\n\"\n",
    "        f\"recon_err={numerical_error[1]:.4e}\\n\"\n",
    "        f\"control_set_file={CTL_TXT.name}\\n\"\n",
    "    )\n",
    "print(f\"[DONE] Risultati scritti in: {RESULT}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5c5ff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Banda 80_150 Hz ===\n",
      "inversion error = 1.49E-10 (<1.00E-08=True)\n",
      "reconstruction error = 3.12E-09 (<1.00E-08=True)\n",
      "  inversion_err      = 1.49e-10\n",
      "  reconstruction_err = 3.12e-09\n",
      "  total energy       = 35.9815\n",
      "    ↳ controllers    = 34.1677\n",
      "    ↳ non-controllers = 1.8139\n",
      "\n",
      "=== Banda 80_250 Hz ===\n",
      "inversion error = 1.41E-10 (<1.00E-08=True)\n",
      "reconstruction error = 3.05E-09 (<1.00E-08=True)\n",
      "  inversion_err      = 1.41e-10\n",
      "  reconstruction_err = 3.05e-09\n",
      "  total energy       = 35.9593\n",
      "    ↳ controllers    = 34.1414\n",
      "    ↳ non-controllers = 1.8179\n",
      "\n",
      "=== Banda 150_250 Hz ===\n",
      "inversion error = 1.30E-10 (<1.00E-08=True)\n",
      "reconstruction error = 2.91E-09 (<1.00E-08=True)\n",
      "  inversion_err      = 1.30e-10\n",
      "  reconstruction_err = 2.91e-09\n",
      "  total energy       = 35.5421\n",
      "    ↳ controllers    = 33.6525\n",
      "    ↳ non-controllers = 1.8896\n",
      "\n",
      "=== Banda 250_500 Hz ===\n",
      "inversion error = 1.22E-10 (<1.00E-08=True)\n",
      "reconstruction error = 4.00E-09 (<1.00E-08=True)\n",
      "  inversion_err      = 1.22e-10\n",
      "  reconstruction_err = 4.00e-09\n",
      "  total energy       = 35.4212\n",
      "    ↳ controllers    = 33.3304\n",
      "    ↳ non-controllers = 2.0908\n",
      "\n",
      "=== Banda 13_30 Hz ===\n",
      "inversion error = 1.50E-10 (<1.00E-08=True)\n",
      "reconstruction error = 2.52E-09 (<1.00E-08=True)\n",
      "  inversion_err      = 1.50e-10\n",
      "  reconstruction_err = 2.52e-09\n",
      "  total energy       = 35.9388\n",
      "    ↳ controllers    = 33.6888\n",
      "    ↳ non-controllers = 2.2500\n",
      "[DONE] summary_all_bands.csv scritto.\n",
      "[DONE] top_nodes_* e barplot dei controller principali per ogni banda.\n"
     ]
    }
   ],
   "source": [
    "# Siccome nel .mat file ho coppie di stati in riferimento a diverse bande di frequenza,\n",
    "# posso ripetere il calcolo del control energy per tutte le bande disponibili.\n",
    "# Per farlo, creo un ciclo che itera sulle coppie di stati nel .mat file,\n",
    "# e salva i risultati in cartelle separate per ogni banda.\n",
    "# Caricamento da MATLAB degli stati x0 = x_interictal & xf = x_ictal\n",
    "STATE_MATE = BASE / \"states_NCT_from_sEEG.mat\"\n",
    "mat_states = loadmat(STATE_MATE)\n",
    "\n",
    "# elenco coppie di stati dal .mat file\n",
    "BANDS = {\n",
    "    \"80_150\": (\"x_interictal_80_150\", \"x_ictal_80_150\"),\n",
    "    \"80_250\": (\"x_interictal_80_250\", \"x_ictal_80_250\"),\n",
    "    \"150_250\": (\"x_interictal_150_250\", \"x_ictal_150_250\"),\n",
    "    \"250_500\": (\"x_interictal_250_500\", \"x_ictal_250_500\"),\n",
    "    \"13_30\": (\"x_interictal_13_30\", \"x_ictal_13_30\"),\n",
    "}\n",
    "\n",
    "# definisco una funzione che esegua tutta la pipeline per una banda:\n",
    "def run_control_energy_for_band(\n",
    "    band_id: str,\n",
    "    name_x0: str,\n",
    "    name_xf: str,\n",
    "    mat_states,\n",
    "    A_norm,\n",
    "    control_set,\n",
    "    time_horizon: float,\n",
    "    system: str,\n",
    "    rho: float,\n",
    "    S: np.ndarray,\n",
    "    initial_mask: np.ndarray,\n",
    "    OUT: Path\n",
    "):\n",
    "    \"\"\"\n",
    "    Esegue tutta la pipeline NCT per una singola banda:\n",
    "    - carica x0, xf dal .mat\n",
    "    - normalizza\n",
    "    - calcola control signals & traiettoria\n",
    "    - calcola energia per nodo\n",
    "    - salva CSV/figure specifiche per quella banda\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Banda {band_id} Hz ===\")\n",
    "    \n",
    "    # 1) prendo gli stati grezzi dal .mat (ROI-level, 850 x 1)\n",
    "    x0_raw = np.asarray(mat_states[name_x0], dtype=float).ravel()\n",
    "    xf_raw = np.asarray(mat_states[name_xf], dtype=float).ravel()\n",
    "\n",
    "    assert x0_raw.shape[0] == A_norm.shape[0] == control_set.shape[0]\n",
    "\n",
    "    # 2) normalizzazione a norma unitaria (come Parkes)\n",
    "    x0 = normalize_state(x0_raw)\n",
    "    xf = normalize_state(xf_raw)\n",
    "\n",
    "    # 3) controllo ottimo\n",
    "    state_traj, control_signals, num_err = get_control_inputs(\n",
    "        A_norm=A_norm,\n",
    "        T=time_horizon,\n",
    "        B=control_set,\n",
    "        x0=x0,\n",
    "        xf=xf,\n",
    "        system=system,\n",
    "        rho=rho,\n",
    "        S=S,\n",
    "    )\n",
    "\n",
    "    # Check inversion / reconstruction errors\n",
    "    thr = 1e-8  #threshold\n",
    "    print(\"inversion error = {:.2E} (<{:.2E}={:})\".format(num_err[0], thr, num_err[0] < thr))\n",
    "    print(\"reconstruction error = {:.2E} (<{:.2E}={:})\".format(num_err[1], thr, num_err[1] < thr))\n",
    "\n",
    "    # 4) energia per nodo\n",
    "    node_energy = integrate_u_compat(control_signals, time_horizon)   # shape (N,)\n",
    "    total_E = float(node_energy.sum())\n",
    "    ctrl_E  = float(node_energy[initial_mask].sum())\n",
    "    nonctrl_E = float(node_energy[~initial_mask].sum())\n",
    "\n",
    "    print(f\"  inversion_err      = {num_err[0]:.2e}\")\n",
    "    print(f\"  reconstruction_err = {num_err[1]:.2e}\")\n",
    "    print(f\"  total energy       = {total_E:.4f}\")\n",
    "    print(f\"    ↳ controllers    = {ctrl_E:.4f}\")\n",
    "    print(f\"    ↳ non-controllers = {nonctrl_E:.4f}\")\n",
    "\n",
    "    # 5) salvataggi per quella banda\n",
    "    result_dir = OUT / \"control_energy\"\n",
    "    result_dir.mkdir(exist_ok=True)\n",
    "        # energia per nodo (.npy + .csv)\n",
    "    np.save(result_dir / f\"node_energy_850_{band_id}.npy\", node_energy)\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"node\": np.arange(1, len(node_energy) + 1),\n",
    "        \"node_energy\": node_energy,\n",
    "        \"is_controller\": initial_mask.astype(int),\n",
    "    }).to_csv(result_dir / f\"node_energy_850_{band_id}.csv\", index=False)\n",
    "\n",
    "    # summary testuale (.txt)\n",
    "    with open(result_dir / f\"summary_{band_id}.txt\", \"w\") as f:\n",
    "        f.write(\n",
    "            f\"band={band_id}\\n\"\n",
    "            f\"energy={total_E:.10f}\\n\"\n",
    "            f\"energy_controllers={ctrl_E:.10f}\\n\"\n",
    "            f\"energy_noncontrollers={nonctrl_E:.10f}\\n\"\n",
    "            f\"inversion_err={num_err[0]:.4e}\\n\"\n",
    "            f\"recon_err={num_err[1]:.4e}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "# ----- Propagazione nel tempo: energia istantanea controller vs non- -----\n",
    "# Per verificare se l'energia si propaga (la dinamica), posso guardare il profilo temporale dell'energia\n",
    "# Dopo aver chiamato get_control_inputs (hao control_signals shape (T_steps, N)), posso:\n",
    "    dt = time_horizon / (control_signals.shape[0] - 1)\n",
    "    t = np.linspace(0, time_horizon, control_signals.shape[0])\n",
    "\n",
    "    # energia istantanea separata per gruppo (controllers vs non)\n",
    "    E_t_ctrl    = np.sum(control_signals[:, initial_mask]**2, axis=1) * dt\n",
    "    E_t_nonctrl = np.sum(control_signals[:, ~initial_mask]**2, axis=1) * dt\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(t, E_t_ctrl, label=\"controllers\")\n",
    "    plt.plot(t, E_t_nonctrl, label=\"non-controllers\")\n",
    "    plt.xlabel(\"time (a.u.)\")\n",
    "    plt.ylabel(\"instantaneous energy (controllers vs non)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT / \"control_energy\" / f\"energy_time_{band_id}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Interpretazione:\n",
    "# - se all'inizio l’energia è concentrata sui controllers e poi cresce sui non-controllers, \n",
    "#   => sto osservando la propagazione lungo la rete; \n",
    "# - se invece l’energia rimane quasi tutta sui controllers,\n",
    "#   => la dinamica non propaga molto l’input di controllo (localizzazione senza coinvolgimento del resto della rete)\n",
    "\n",
    "    # --- HEATMAP DI |u(t)| ---\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.imshow(control_signals.T, aspect=\"auto\", interpolation=\"nearest\",\n",
    "            extent=[t[0], t[-1], 0, N])\n",
    "    plt.colorbar(label=\"u(t)\")\n",
    "    plt.xlabel(\"time (a.u.)\")\n",
    "    plt.ylabel(\"node index\")\n",
    "    plt.title(f\"Control signals per node — {band_id} Hz\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT / \"control_energy\" / f\"u_heatmap_{band_id}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# ritorno i risultati per analisi cross-banda\n",
    "    return {\n",
    "        \"band\": band_id,\n",
    "        \"total_E\": total_E,\n",
    "        \"ctrl_E\": ctrl_E,\n",
    "        \"nonctrl_E\": nonctrl_E,\n",
    "        \"errors\": num_err,\n",
    "        \"node_energy\": node_energy,\n",
    "        \"state_traj\": state_traj,\n",
    "        \"u\": control_signals,\n",
    "        \"x0_raw\": x0_raw,\n",
    "        \"xf_raw\": xf_raw,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============== Loop su tutte le bande ===============\n",
    "results = []\n",
    "for band_id, (name_x0, name_xf) in BANDS.items():\n",
    "    res = run_control_energy_for_band(\n",
    "        band_id=band_id,\n",
    "        name_x0=name_x0,\n",
    "        name_xf=name_xf,\n",
    "        mat_states=mat_states,\n",
    "        A_norm=A_norm,\n",
    "        control_set=control_set,\n",
    "        time_horizon=time_horizon,\n",
    "        system=system,\n",
    "        rho=rho,\n",
    "        S=S,\n",
    "        initial_mask=initial_mask,\n",
    "        OUT=OUT,\n",
    "    )\n",
    "    results.append(res)\n",
    "\n",
    "    # Riepilogo generale in un CSV\n",
    "summary_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"band\": r[\"band\"],\n",
    "            \"total_E\": r[\"total_E\"],\n",
    "            \"ctrl_E\": r[\"ctrl_E\"],\n",
    "            \"nonctrl_E\": r[\"nonctrl_E\"],\n",
    "            \"inv_err\": r[\"errors\"][0],\n",
    "            \"rec_err\": r[\"errors\"][1],\n",
    "        }\n",
    "        for r in results\n",
    "    ]\n",
    ")\n",
    "summary_df.to_csv(OUT / \"control_energy\" / \"summary_all_bands.csv\", index=False)\n",
    "print(\"[DONE] summary_all_bands.csv scritto.\")\n",
    "\n",
    "# così per ogni banda ho un file node_energy_850_<banda>.csv con l'energia per ROI/nodo\n",
    "# e un summary_<banda>.txt + summary_all_bands.csv che dice il totale dell'energia e quanta nei nodi controllori.\n",
    "\n",
    "# ======================================================================\n",
    "#        Estrazione e visualizzazione dei principali controller\n",
    "# ======================================================================\n",
    "\n",
    "# Carico gli ID delle ROI (B0_TXT)\n",
    "b0_ids_vec = pd.read_csv(\n",
    "    B0_TXT, sep=r\"[\\s,;]+\", engine=\"python\", header=None, comment=\"#\"\n",
    ").iloc[:, 0].astype(int).to_numpy()\n",
    "assert len(b0_ids_vec) == N\n",
    "\n",
    "top_k = 100  # quante ROI considero come \"principali\" per banda\n",
    "\n",
    "result_dir = OUT / \"control_energy\"\n",
    "result_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for res in results:\n",
    "    band = res[\"band\"]\n",
    "    E = res[\"node_energy\"]  # energia per ROI in quella banda\n",
    "\n",
    "    # ---- (a) top nodi tra TUTTE le ROI ----\n",
    "    idx_sorted = np.argsort(E)[::-1]  # indici ordinati per energia decrescente\n",
    "    idx_top = idx_sorted[:top_k]\n",
    "\n",
    "    df_top_all = pd.DataFrame(\n",
    "        {\n",
    "            \"rank\": np.arange(1, len(idx_top) + 1),\n",
    "            \"roi_index\": idx_top + 1,  # indice 1-based\n",
    "            \"roi_id\": b0_ids_vec[idx_top],  # ID usato nel connectome\n",
    "            \"is_controller\": initial_mask[idx_top],  # True se ROI ha contatti sEEG\n",
    "            \"node_energy\": E[idx_top],\n",
    "        }\n",
    "    )\n",
    "    df_top_all.to_csv(\n",
    "        result_dir / f\"top_nodes_all_{band}.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    # ---- (b) top nodi SOLO tra i controller (ROI con sEEG) ----\n",
    "    idx_ctrl = np.where(initial_mask)[0]  # indici dei controller forti\n",
    "    E_ctrl = E[idx_ctrl]\n",
    "    order_ctrl_local = np.argsort(E_ctrl)[::-1]\n",
    "    idx_top_ctrl = idx_ctrl[order_ctrl_local[:top_k]]\n",
    "\n",
    "    df_top_ctrl = pd.DataFrame(\n",
    "        {\n",
    "            \"rank\": np.arange(1, len(idx_top_ctrl) + 1),\n",
    "            \"roi_index\": idx_top_ctrl + 1,\n",
    "            \"roi_id\": b0_ids_vec[idx_top_ctrl],\n",
    "            \"node_energy\": E[idx_top_ctrl],\n",
    "        }\n",
    "    )\n",
    "    df_top_ctrl.to_csv(\n",
    "        result_dir / f\"top_nodes_controllers_{band}.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    # ---- (c) bar-plot dei principali controller ----\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(np.arange(len(idx_top_ctrl)), E[idx_top_ctrl])\n",
    "    plt.xticks(\n",
    "        np.arange(len(idx_top_ctrl)),\n",
    "        [str(b0_ids_vec[i]) for i in idx_top_ctrl],\n",
    "        rotation=90,\n",
    "    )\n",
    "    plt.ylabel(\"node control energy\")\n",
    "    plt.xlabel(\"ROI ID (controllers)\")\n",
    "    plt.title(f\"Top {len(idx_top_ctrl)} controller nodes — banda {band} Hz\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        result_dir / f\"top_controllers_bar_{band}.png\",\n",
    "        dpi=300,\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "print(\"[DONE] top_nodes_* e barplot dei controller principali per ogni banda.\")\n",
    "\n",
    "# per ogni banda ottengo:\n",
    "# top_nodes_all_<band>.csv → i nodi con energia più alta nell’intera rete,\n",
    "# top_nodes_controllers_<band>.csv → tra le 77 ROI con contatti sEEG, quali sono i controller principali,\n",
    "# top_controllers_bar_<band>.png → grafico a barre dell’energia dei controller principali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81535e2",
   "metadata": {},
   "source": [
    "Post-analisi energia + export CSV→XLSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "adbe9f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energia iniziale: 17.372077768890428\n",
      "Energia target  : 18.340027276125035\n",
      "Energia bystanders: 16.936328053630444\n",
      "Energia totale: 35.98154033573532\n",
      "[SAVE] node_energy_by_group.csv salvato in c:\\Users\\barba\\Visual Studio - Github\\NCT\\output\n",
      "Creato: A_struct_clean.xlsx\n",
      "Creato: bbox_summary.xlsx\n",
      "Creato: bipolar_top1_outliers_gt_6.0mm.xlsx\n",
      "Creato: bi_k1_top1_outliers_gt_6.0mm.xlsx\n",
      "Creato: bi_k3_top1_outliers_gt_6.0mm.xlsx\n",
      "Creato: B_diag_matrix.xlsx\n",
      "Creato: B_vector_diag.xlsx\n",
      "Creato: check_bipolar_midpoint.xlsx\n",
      "Creato: contact2roi_BI_top1.xlsx\n",
      "Creato: contact2roi_MONO_top1.xlsx\n",
      "Creato: contacts_vs_B0_MNI_nearest.xlsx\n",
      "Creato: controlset_map_bipolar_k1.xlsx\n",
      "Creato: controlset_map_monopolar_k1.xlsx\n",
      "Creato: controlset_map_monopolar_k3.xlsx\n",
      "Creato: controlset_top1_outliers_gt_6.0mm.xlsx\n",
      "Creato: control_energy_qc_summary.xlsx\n",
      "Creato: monopolar_top1_outliers_gt_6.0mm.xlsx\n",
      "Creato: mono_k1_top1_outliers_gt_6.0mm.xlsx\n",
      "Creato: mono_k3_top1_outliers_gt_6.0mm.xlsx\n",
      "Creato: node_energy_by_group.xlsx\n",
      "Creato: sEEG_bipolar_aligned_to_B0.xlsx\n",
      "Creato: sEEG_bipolar_aligned_to_MNI.xlsx\n",
      "Creato: sEEG_monopolar_aligned_to_B0.xlsx\n",
      "Creato: sEEG_monopolar_aligned_to_MNI.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ============================ Analisi energia per gruppo ===================================\n",
    "# maschere: initial / target / bystanders\n",
    "mask_init = initial_state_raw > np.median(initial_state_raw) # \n",
    "mask_targ = target_state_raw > np.median(target_state_raw)\n",
    "mask_byst = ~(mask_init | mask_targ)\n",
    "\n",
    "print(\"Energia iniziale:\", float(node_energy[mask_init].sum()))\n",
    "print(\"Energia target  :\", float(node_energy[mask_targ].sum()))\n",
    "print(\"Energia bystanders:\", float(node_energy[mask_byst].sum()))\n",
    "print(\"Energia totale:\", float(node_energy.sum()))\n",
    "\n",
    "# (opzionale ma utile) mappa ROI-ID → energia e gruppo, così ho un CSV pronto\n",
    "b0_ids_vec = pd.read_csv(\n",
    "    B0_TXT, sep=r\"[\\s,;]+\", engine=\"python\", header=None, comment=\"#\"\n",
    ").iloc[:, 0].astype(int).to_numpy()\n",
    "assert len(b0_ids_vec) == len(node_energy), \"Numero ID B0 diverso da #ROI dell’energia\"\n",
    "\n",
    "group = np.full(len(node_energy), \"bystander\", dtype=object)\n",
    "group[mask_init] = \"initial\"\n",
    "group[mask_targ] = \"target\"\n",
    "\n",
    "dfE = pd.DataFrame({\"roi_id\": b0_ids_vec, \"node_energy\": node_energy, \"group\": group})\n",
    "dfE.to_csv(OUT / \"node_energy_by_group.csv\", index=False)\n",
    "print(\"[SAVE] node_energy_by_group.csv salvato in\", OUT)\n",
    "\n",
    "# istogramma\n",
    "plt.figure()\n",
    "plt.hist(node_energy, bins=30)\n",
    "plt.xlabel(\"Node control energy\")\n",
    "plt.ylabel(\"Conteggio ROI\")\n",
    "plt.title(\"Distribuzione energia di controllo (850 ROI)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT / \"hist_node_energy.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "# ============================================================\n",
    "\n",
    "#conversione file: un .xlsx per ogni .csv nella cartella output\n",
    "for csv in OUT.glob(\"*.csv\"):\n",
    "    try:\n",
    "        df = pd.read_csv(csv)  # se servisse, aggiungo: sep=\",\" , encoding=\"utf-8\"\n",
    "        xlsx_path = csv.with_suffix(\".xlsx\")\n",
    "        df.to_excel(xlsx_path, index=False, engine=\"openpyxl\")\n",
    "        print(f\"Creato: {xlsx_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore su {csv.name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nct_env)",
   "language": "python",
   "name": "nct_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
